{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Renewable Energy Forecasting - Data Cleaning & Preprocessing\n",
        "## AICTE/Shell/Edunet Internship Project\n",
        "\n",
        "This notebook demonstrates complete data cleaning and preprocessing workflow for renewable energy forecasting.\n",
        "\n",
        "**Objective**: Predict solar or wind power generation based on weather data using machine learning.\n",
        "\n",
        "**Dataset**: Renewable Energy Forecasting Dataset (1000 samples, 10 features + target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print('All libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape: (1000, 11)\n",
            "\n",
            "First 10 rows:\n",
            "    Feature_1  Feature_2  Feature_3  Feature_4  Feature_5    Feature_6  \\\n",
            "0  374.602665   3.184427  17.851171  70.452725   0.871599   989.363552   \n",
            "1  950.719235   8.357564  17.409364  79.751105   0.941630   997.343566   \n",
            "2  732.020742  13.157715  37.187637  38.785092   0.928048  1035.454739   \n",
            "3  598.698618  11.117261  17.486386  66.865557   0.746170   984.000439   \n",
            "4  156.103039  12.195137  18.158492  62.880949   0.744775  1036.964968   \n",
            "5  156.078921  10.052359        NaN  82.462278   0.780452   958.813443   \n",
            "6   58.177804  10.538010  23.492195  87.956530   0.808322  1027.679844   \n",
            "7  866.189528  12.813337  33.301317  20.911758   0.822537  1034.754763   \n",
            "8  601.154900   4.120186  11.960985  70.551494   0.903909   968.181765   \n",
            "9  708.101771   7.596662  24.627136  23.887685   0.717004   993.034653   \n",
            "\n",
            "    Feature_7  Feature_8  Feature_9  Feature_10    Target  \n",
            "0  233.372503   3.879945  33.216075    0.913578  0.607938  \n",
            "1   62.059090  18.677253  31.236980    0.525360  0.813446  \n",
            "2  314.062043  83.124581  -4.254748    0.724910  0.636696  \n",
            "3  220.721846  76.676836  45.354344    0.436048  0.477168  \n",
            "4   56.593398  35.064269  24.108332    0.630035  0.490076  \n",
            "5  346.441701  37.681063  11.823531    0.340576  0.450633  \n",
            "6  186.611567  53.355443  35.392315    0.301159  0.393083  \n",
            "7   26.243443   0.024100   5.441928    0.578230  0.793848  \n",
            "8  225.659846  24.124434  31.610590    0.498130  0.774560  \n",
            "9   91.151607  20.823181  -7.617328    0.579536  0.745209  \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 11 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Feature_1   984 non-null    float64\n",
            " 1   Feature_2   1000 non-null   float64\n",
            " 2   Feature_3   986 non-null    float64\n",
            " 3   Feature_4   1000 non-null   float64\n",
            " 4   Feature_5   988 non-null    float64\n",
            " 5   Feature_6   1000 non-null   float64\n",
            " 6   Feature_7   1000 non-null   float64\n",
            " 7   Feature_8   1000 non-null   float64\n",
            " 8   Feature_9   1000 non-null   float64\n",
            " 9   Feature_10  1000 non-null   float64\n",
            " 10  Target      1000 non-null   float64\n",
            "dtypes: float64(11)\n",
            "memory usage: 86.1 KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Load raw data\n",
        "df_raw = pd.read_csv('renewable_energy_raw.csv')\n",
        "\n",
        "print(\"Dataset Shape:\", df_raw.shape)\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(df_raw.head(10))\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df_raw.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXPLORATORY DATA ANALYSIS\n",
            "============================================================\n",
            "\n",
            "1. MISSING VALUES:\n",
            "   Column  Missing_Count  Missing_Percent\n",
            "Feature_1             16              1.6\n",
            "Feature_3             14              1.4\n",
            "Feature_5             12              1.2\n",
            "\n",
            "2. DESCRIPTIVE STATISTICS:\n",
            "        Feature_1    Feature_2   Feature_3    Feature_4   Feature_5  \\\n",
            "count  984.000000  1000.000000  986.000000  1000.000000  988.000000   \n",
            "mean   488.592148     9.268573   25.045241    56.778129    0.848118   \n",
            "std    292.676737    10.919924    8.736342    21.486701    0.086269   \n",
            "min      4.731560     0.546665   10.000349    20.049004    0.700009   \n",
            "25%    230.695884     4.081459   17.805702    38.142558    0.772617   \n",
            "50%    494.158540     8.232562   24.950623    56.320946    0.846584   \n",
            "75%    740.882540    11.719568   32.768441    75.315561    0.922501   \n",
            "max    999.717702    96.795279   39.934626    94.966828    0.999325   \n",
            "\n",
            "         Feature_6    Feature_7    Feature_8    Feature_9   Feature_10  \\\n",
            "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
            "mean    999.846449   177.591182    48.237062    19.189802     0.496795   \n",
            "std      28.919481   104.304111    28.491440    16.934071     0.282153   \n",
            "min     950.618382     0.514204     0.024100    -9.985424     0.000053   \n",
            "25%     974.938000    86.479799    23.598929     4.963023     0.264144   \n",
            "50%     999.040282   174.471489    48.399070    18.722057     0.495187   \n",
            "75%    1023.985178   272.327165    71.446625    32.954298     0.736217   \n",
            "max    1049.935030   359.805845    99.890470    49.760767     0.999505   \n",
            "\n",
            "            Target  \n",
            "count  1000.000000  \n",
            "mean      0.603216  \n",
            "std       0.145296  \n",
            "min       0.239298  \n",
            "25%       0.492899  \n",
            "50%       0.604015  \n",
            "75%       0.708834  \n",
            "max       0.981112  \n",
            "\n",
            "3. DATA TYPES:\n",
            "Feature_1     float64\n",
            "Feature_2     float64\n",
            "Feature_3     float64\n",
            "Feature_4     float64\n",
            "Feature_5     float64\n",
            "Feature_6     float64\n",
            "Feature_7     float64\n",
            "Feature_8     float64\n",
            "Feature_9     float64\n",
            "Feature_10    float64\n",
            "Target        float64\n",
            "dtype: object\n",
            "\n",
            "4. DUPLICATE ROWS: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Missing values\n",
        "print(\"\\n1. MISSING VALUES:\")\n",
        "missing_data = df_raw.isnull().sum()\n",
        "missing_percent = (missing_data / len(df_raw)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': df_raw.columns,\n",
        "    'Missing_Count': missing_data.values,\n",
        "    'Missing_Percent': missing_percent.values\n",
        "})\n",
        "print(missing_df[missing_df['Missing_Count'] > 0].to_string(index=False))\n",
        "\n",
        "# Descriptive Statistics\n",
        "print(\"\\n2. DESCRIPTIVE STATISTICS:\")\n",
        "print(df_raw.describe())\n",
        "\n",
        "# Data Types\n",
        "print(\"\\n3. DATA TYPES:\")\n",
        "print(df_raw.dtypes)\n",
        "\n",
        "# Duplicate rows\n",
        "print(f\"\\n4. DUPLICATE ROWS: {df_raw.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Cleaning - Step 1: Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: HANDLING MISSING VALUES\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'SimpleImputer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m df_cleaned = df_raw.copy()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Use median imputation for numerical features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m imputer = \u001b[43mSimpleImputer\u001b[49m(strategy=\u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m numeric_cols = [\u001b[33m'\u001b[39m\u001b[33mFeature_1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_5\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMissing values before imputation:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'SimpleImputer' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: HANDLING MISSING VALUES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df_cleaned = df_raw.copy()\n",
        "\n",
        "# Use median imputation for numerical features\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "numeric_cols = ['Feature_1', 'Feature_3', 'Feature_5']\n",
        "\n",
        "print(f\"\\nMissing values before imputation:\")\n",
        "print(df_cleaned[numeric_cols].isnull().sum())\n",
        "\n",
        "df_cleaned[numeric_cols] = imputer.fit_transform(df_cleaned[numeric_cols])\n",
        "\n",
        "print(f\"\\nMissing values after imputation:\")\n",
        "print(df_cleaned[numeric_cols].isnull().sum())\n",
        "print(\"\\n✓ Missing values successfully handled using median imputation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Cleaning - Step 2: Detect and Handle Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: DETECTING AND HANDLING OUTLIERS (IQR Method)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def remove_outliers_iqr(data, column, multiplier=1.5):\n",
        "    \"\"\"Detect outliers using IQR method\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - multiplier * IQR\n",
        "    upper_bound = Q3 + multiplier * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "outlier_count = 0\n",
        "for col in ['Feature_2', 'Feature_1', 'Feature_3']:\n",
        "    outliers, lower, upper = remove_outliers_iqr(df_cleaned, col)\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"\\n{col}: Found {len(outliers)} outliers\")\n",
        "        print(f\"  Valid Range: [{lower:.2f}, {upper:.2f}]\")\n",
        "        # Cap outliers instead of removing\n",
        "        df_cleaned[col] = df_cleaned[col].clip(lower, upper)\n",
        "        outlier_count += len(outliers)\n",
        "\n",
        "print(f\"\\n✓ Total outliers handled: {outlier_count} values were capped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Cleaning - Step 3: Remove Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: REMOVING DUPLICATES\n",
            "============================================================\n",
            "\n",
            "Duplicate rows found and removed: 0\n",
            "Rows before: 1000, After: 1000\n",
            "\n",
            "✓ Duplicate removal complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: REMOVING DUPLICATES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "duplicates_before = len(df_cleaned)\n",
        "df_cleaned = df_cleaned.drop_duplicates()\n",
        "duplicates_removed = duplicates_before - len(df_cleaned)\n",
        "\n",
        "print(f\"\\nDuplicate rows found and removed: {duplicates_removed}\")\n",
        "print(f\"Rows before: {duplicates_before}, After: {len(df_cleaned)}\")\n",
        "print(f\"\\n✓ Duplicate removal complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Cleaning - Step 4: Data Type Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: DATA TYPE VALIDATION\n",
            "============================================================\n",
            "\n",
            "Data Types before conversion:\n",
            "Feature_1     float64\n",
            "Feature_2     float64\n",
            "Feature_3     float64\n",
            "Feature_4     float64\n",
            "Feature_5     float64\n",
            "Feature_6     float64\n",
            "Feature_7     float64\n",
            "Feature_8     float64\n",
            "Feature_9     float64\n",
            "Feature_10    float64\n",
            "Target        float64\n",
            "dtype: object\n",
            "\n",
            "✓ All numeric columns validated as float64\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: DATA TYPE VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nData Types before conversion:\")\n",
        "print(df_cleaned.dtypes)\n",
        "\n",
        "# Ensure all numeric columns are float64\n",
        "numeric_features = [col for col in df_cleaned.columns if col.startswith('Feature')] + ['Target']\n",
        "for col in numeric_features:\n",
        "    df_cleaned[col] = df_cleaned[col].astype('float64')\n",
        "\n",
        "print(f\"\\n✓ All numeric columns validated as float64\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Data Preprocessing - Step 5: Feature Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: FEATURE NORMALIZATION (Min-Max Scaling)\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'MinMaxScaler' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize MinMax scaler\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m scaler = \u001b[43mMinMaxScaler\u001b[49m(feature_range=(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m      8\u001b[39m features_to_scale = [\u001b[33m'\u001b[39m\u001b[33mFeature_1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_4\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      9\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33mFeature_5\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_6\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_7\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_8\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     10\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33mFeature_9\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFeature_10\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mScaling the following features to range [0, 1]:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'MinMaxScaler' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: FEATURE NORMALIZATION (Min-Max Scaling)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize MinMax scaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "features_to_scale = ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', \n",
        "                     'Feature_5', 'Feature_6', 'Feature_7', 'Feature_8', \n",
        "                     'Feature_9', 'Feature_10']\n",
        "\n",
        "print(f\"\\nScaling the following features to range [0, 1]:\")\n",
        "print(features_to_scale)\n",
        "\n",
        "df_cleaned[features_to_scale] = scaler.fit_transform(df_cleaned[features_to_scale])\n",
        "\n",
        "print(f\"\\n✓ Features successfully normalized to [0, 1]\")\n",
        "print(f\"\\nNormalized data statistics:\")\n",
        "print(df_cleaned[features_to_scale].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final Data Quality Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLEANED DATA SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nFinal Dataset Shape: {df_cleaned.shape}\")\n",
        "print(f\"Total Rows: {len(df_cleaned)}\")\n",
        "print(f\"Total Columns: {len(df_cleaned.columns)}\")\n",
        "print(f\"Missing Values: {df_cleaned.isnull().sum().sum()}\")\n",
        "\n",
        "print(f\"\\nFirst 5 rows of cleaned data:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "print(f\"\\nDescriptive Statistics:\")\n",
        "print(df_cleaned.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Data Comparison: Raw vs Cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "DATA COMPARISON: RAW vs CLEANED\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'outlier_count' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDATA COMPARISON: RAW vs CLEANED\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m comparison_summary = pd.DataFrame({\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMetric\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mTotal Rows\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTotal Columns\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMissing Values\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDuplicate Rows\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mOutliers Handled\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRaw Data\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(df_raw), \u001b[38;5;28mlen\u001b[39m(df_raw.columns), df_raw.isnull().sum().sum(), \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCleaned Data\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(df_cleaned), \u001b[38;5;28mlen\u001b[39m(df_cleaned.columns), df_cleaned.isnull().sum().sum(), duplicates_removed, \u001b[43moutlier_count\u001b[49m]\n\u001b[32m      9\u001b[39m })\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(comparison_summary.to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "\u001b[31mNameError\u001b[39m: name 'outlier_count' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA COMPARISON: RAW vs CLEANED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_summary = pd.DataFrame({\n",
        "    'Metric': ['Total Rows', 'Total Columns', 'Missing Values', 'Duplicate Rows', 'Outliers Handled'],\n",
        "    'Raw Data': [len(df_raw), len(df_raw.columns), df_raw.isnull().sum().sum(), 0, 0],\n",
        "    'Cleaned Data': [len(df_cleaned), len(df_cleaned.columns), df_cleaned.isnull().sum().sum(), duplicates_removed, outlier_count]\n",
        "})\n",
        "\n",
        "print(comparison_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned data\n",
        "df_cleaned.to_csv('renewable_energy_cleaned.csv', index=False)\n",
        "print(\"✓ Cleaned data saved as 'renewable_energy_cleaned.csv'\")\n",
        "\n",
        "# Save comparison summary\n",
        "comparison_summary.to_csv('data_preprocessing_summary.csv', index=False)\n",
        "print(\"✓ Preprocessing summary saved as 'data_preprocessing_summary.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA PREPROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nFiles generated:\")\n",
        "print(\"1. renewable_energy_raw.csv - Original raw dataset\")\n",
        "print(\"2. renewable_energy_cleaned.csv - Cleaned and processed dataset\")\n",
        "print(\"3. data_preprocessing_summary.csv - Comparison summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Next Steps for ML Model Development\n",
        "\n",
        "Now that we have clean, preprocessed data, the next steps are:\n",
        "\n",
        "1. **Train-Test Split**: Split data into training and testing sets (80-20 or 70-30)\n",
        "2. **Feature Engineering**: Create new features if needed (e.g., lagged features for time-series)\n",
        "3. **Model Selection**: Choose appropriate models:\n",
        "   - LSTM (Long Short-Term Memory) for time-series forecasting\n",
        "   - Random Forest for non-linear relationships\n",
        "   - Gradient Boosting for high accuracy\n",
        "   - XGBoost for improved performance\n",
        "4. **Model Training**: Train models on the cleaned dataset\n",
        "5. **Model Evaluation**: Use metrics like RMSE, MAE, R² Score\n",
        "6. **Hyperparameter Tuning**: Optimize model parameters using GridSearchCV or RandomizedSearchCV\n",
        "7. **Predictions & Visualization**: Generate predictions and visualize results\n",
        "\n",
        "**Good luck with your AICTE/Shell/Edunet internship project!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
